{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":97669,"databundleVersionId":11615683,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:05:40.196623Z","iopub.execute_input":"2025-04-24T08:05:40.197236Z","iopub.status.idle":"2025-04-24T08:05:40.203165Z","shell.execute_reply.started":"2025-04-24T08:05:40.197208Z","shell.execute_reply":"2025-04-24T08:05:40.202457Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/classification-of-math-problems-by-kasut-academy/sample_submission.csv\n/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv\n/kaggle/input/classification-of-math-problems-by-kasut-academy/test.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#Observing the dataset\n\nimport pandas as pd\n\ntrain_path = '/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv'\ntest_path = '/kaggle/input/classification-of-math-problems-by-kasut-academy/test.csv'\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\n\ntrain.rename(columns={\"Question\": \"question\"}, inplace=True)\ntest.rename(columns={\"Question\": \"question\"}, inplace=True)\n\nprint('first 5 rows of the training data')\ntrain.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:05:43.593370Z","iopub.execute_input":"2025-04-24T08:05:43.593856Z","iopub.status.idle":"2025-04-24T08:05:43.657764Z","shell.execute_reply.started":"2025-04-24T08:05:43.593823Z","shell.execute_reply":"2025-04-24T08:05:43.656948Z"}},"outputs":[{"name":"stdout","text":"first 5 rows of the training data\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                            question  label\n0  A solitaire game is played as follows.  Six di...      3\n1  2. The school table tennis championship was he...      5\n2  Given that $x, y,$ and $z$ are real numbers th...      0\n3  $25 \\cdot 22$ Given three distinct points $P\\l...      1\n4  I am thinking of a five-digit number composed ...      5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A solitaire game is played as follows.  Six di...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2. The school table tennis championship was he...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Given that $x, y,$ and $z$ are real numbers th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>$25 \\cdot 22$ Given three distinct points $P\\l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I am thinking of a five-digit number composed ...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"#Installing transformers and datasets\n!pip install -q transformers datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:05:48.920658Z","iopub.execute_input":"2025-04-24T08:05:48.921183Z","iopub.status.idle":"2025-04-24T08:05:52.340317Z","shell.execute_reply.started":"2025-04-24T08:05:48.921152Z","shell.execute_reply":"2025-04-24T08:05:52.339483Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer\n)\nfrom datasets import Dataset\n\n# Step 1: Prepare your original full training data (pandas DataFrame)\n# Assume your labeled data is in a dataframe called `train` with 'question' and 'label' columns\ntrain_df, eval_df = train_test_split(train, test_size=0.1, random_state=42, stratify=train['label'])\n\n# Convert to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df)\neval_dataset = Dataset.from_pandas(eval_df)\ntest_dataset = Dataset.from_pandas(test)  # Unlabeled test data from Kaggle\n\n# Step 2: Tokenization\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_function(example):\n    return tokenizer(example[\"question\"], padding=\"max_length\", truncation=True)\n\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:33:45.567267Z","iopub.execute_input":"2025-04-24T09:33:45.567566Z","iopub.status.idle":"2025-04-24T09:33:51.132071Z","shell.execute_reply.started":"2025-04-24T09:33:45.567546Z","shell.execute_reply":"2025-04-24T09:33:51.131114Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9170 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3ee40fbf944f72a7355701dcb12632"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1019 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68452d7a0d4649119a55905866acd472"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3044 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1220c6a50f764242b26418339a908823"}},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:33:57.815017Z","iopub.execute_input":"2025-04-24T09:33:57.815648Z","iopub.status.idle":"2025-04-24T09:33:57.819896Z","shell.execute_reply.started":"2025-04-24T09:33:57.815622Z","shell.execute_reply":"2025-04-24T09:33:57.819174Z"}},"outputs":[{"name":"stdout","text":"4.51.1\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"!pip install -U transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:21:59.707091Z","iopub.execute_input":"2025-04-24T08:21:59.707400Z","iopub.status.idle":"2025-04-24T08:22:03.301093Z","shell.execute_reply.started":"2025-04-24T08:21:59.707379Z","shell.execute_reply":"2025-04-24T08:22:03.300150Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nimport torch\n\n# Step 3: Load model and send to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=8)\nmodel.to(device)\n\n# Step 4: Metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average='weighted')\n    return {\"accuracy\": acc, \"f1\": f1}\n\n# Step 5: Training arguments (manual eval — no evaluation_strategy)\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=1,\n    save_steps=500,\n    do_train=True,\n    do_eval=False,           # 👈 Disable built-in eval\n    report_to=\"none\"\n)\n\n# Step 6: Trainer (no eval_dataset since we eval manually)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n)\n\n# Step 7: Train\ntrainer.train()\n\n# Step 8: Manually Evaluate on validation set\noutputs = trainer.predict(tokenized_eval_dataset)\npredictions = np.argmax(outputs.predictions, axis=1)\nlabels = outputs.label_ids\n\n# Step 9: Calculate metrics\nacc = accuracy_score(labels, predictions)\nf1 = f1_score(labels, predictions, average=\"weighted\")\nprint(f\"Manual Evaluation Metrics:\\n  Accuracy: {acc:.4f}\\n  F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:36:03.626996Z","iopub.execute_input":"2025-04-24T09:36:03.627523Z","iopub.status.idle":"2025-04-24T10:01:38.121399Z","shell.execute_reply.started":"2025-04-24T09:36:03.627500Z","shell.execute_reply":"2025-04-24T10:01:38.120628Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1722' max='1722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1722/1722 25:13, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.070200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.680800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.544300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.305800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.270900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.046600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.107500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.085700</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.040800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.056500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.883200</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.028400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.911200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.738000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.780000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.863000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.022900</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.771300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.793000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.761600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.683500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.725700</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.726300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.760100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.644800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.933900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.714900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.526000</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.763700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.842900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.864900</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.513700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.669800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.630200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.708700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.464500</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.496300</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.539200</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.607300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.619700</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.635700</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.631200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.634200</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.750100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.612500</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.510600</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.605100</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.574900</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.611800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.569700</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.676300</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.704800</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.674500</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.540200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.672200</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.546500</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.648000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.529300</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.349300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.426000</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.357100</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.455400</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.358500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.454400</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.385900</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.448000</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.384200</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.280900</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.475600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.404300</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.428800</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.416000</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.383500</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.311200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.418400</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.465100</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.530700</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.543600</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.506000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.405700</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.398700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.329900</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.324500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.473400</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.392900</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.535900</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.439700</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.349500</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.506600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.433900</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.483100</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.459700</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.259000</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.465500</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.445500</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.491500</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.381200</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.280400</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.352400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.320400</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.403600</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.503000</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.347700</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.351600</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.303400</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.390000</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.413200</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.346400</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.357800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.373500</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.340600</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.408900</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.459500</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.422300</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.295200</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.240400</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.275700</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.303000</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.356800</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.175600</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.167500</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.213600</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.230800</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.270300</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.196800</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.155000</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.221800</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.138800</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.330100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.279000</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.133900</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.190500</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.131300</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.264700</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.252500</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.250300</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.180900</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.184100</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.316400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.195900</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.180000</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.172600</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.248800</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.268100</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.153700</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.223600</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.201200</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.307000</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.252000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.080300</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.248000</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.174600</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.186600</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.180500</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.356100</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.138900</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.146400</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.192000</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.272100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.101800</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.135500</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.250100</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.205900</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.206200</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.112300</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.265500</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.102700</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.285300</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.147500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.200100</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.141600</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.157000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Manual Evaluation Metrics:\n  Accuracy: 0.8292\n  F1 Score: 0.8284\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Step 8: Predict on unlabeled test set\npredictions = trainer.predict(tokenized_test_dataset)\npred_labels = np.argmax(predictions.predictions, axis=1)\n\n# Step 9: Prepare submission\ntest[\"label\"] = pred_labels\nsubmission = test[[\"id\", \"label\"]]  # Assuming your test set has an 'id' column\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T10:04:10.379577Z","iopub.execute_input":"2025-04-24T10:04:10.379875Z","iopub.status.idle":"2025-04-24T10:05:06.773219Z","shell.execute_reply.started":"2025-04-24T10:04:10.379858Z","shell.execute_reply":"2025-04-24T10:05:06.772557Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Submission saved to submission.csv\n","output_type":"stream"}],"execution_count":45}]}